#!/usr/bin/env python3
"""
arXiv Monitor - è·å–æ–°è®ºæ–‡å¹¶åˆ›å»º GitHub Issues

Usage:
    # å¸¦ Tokenï¼šè·å–è®ºæ–‡å¹¶åˆ›å»º Issue
    python scripts/monitor_arxiv.py \
        --token "ghp_xxx" \
        --repo "owner/repo" \
        --categories "cs.AI,cs.LG,cs.CL" \
        --max-papers 5

    # ä¸å¸¦ Tokenï¼šä»…è·å–è®ºæ–‡åˆ—è¡¨åˆ° JSON
    python scripts/monitor_arxiv.py \
        --output /tmp/papers.json \
        --categories "cs.AI"
"""

import argparse
import json
import re
import sys
import time
from datetime import datetime
from typing import Any

import feedparser
from github import Github


def parse_arxiv_date(date_str: str) -> str:
    """è§£æ arXiv æ—¥æœŸæ ¼å¼"""
    try:
        dt = datetime.strptime(date_str[:19], "%Y-%m-%dT%H:%M:%S")
        return dt.strftime("%Y-%m-%d")
    except (ValueError, TypeError):
        return date_str[:10] if date_str else "Unknown"


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ä¸­çš„å¤šä½™ç©ºç™½"""
    return re.sub(r"\s+", " ", text).strip()


def truncate_text(text: str, max_length: int = 1500) -> str:
    """æˆªæ–­æ–‡æœ¬"""
    if len(text) <= max_length:
        return text
    return text[:max_length].rsplit(".", 1)[0] +..."


def fetch_papers(categories: list[str], last_scan: str, max_papers: int = 10) -> list[dict[str, Any]]:
    """è·å– arXiv æ–°è®ºæ–‡"""
    try:
        last_scan_dt = datetime.strptime(last_scan[:19], "%Y-%m-%dT%H:%M:%S")
        last_scan_timestamp = last_scan_dt.timestamp()
    except (ValueError, TypeError):
        last_scan_timestamp = 0

    all_papers = []

    for category in categories:
        print(f"ğŸ“¥ è·å– {category} åˆ†ç±»...")

        base_url = "http://export.arxiv.org/api/query"
        params = {
            "search_query": f"cat:{category}",
            "sortBy": "submittedDate",
            "sortOrder": "descending",
            "max_results": max_papers * 3,
        }
        url = f"{base_url}?{ '&'.join(f'{k}={v}' for k,v in params.items()) }"

        try:
            response = feedparser.parse(url)

            for entry in response.entries:
                try:
                    published_timestamp = datetime.strptime(
                        entry.get("published", "")[:19], "%Y-%m-%dT%H:%M:%S"
                    ).timestamp()
                except (ValueError, TypeError):
                    continue

                if published_timestamp <= last_scan_timestamp:
                    continue

                authors = ", ".join(
                    a.get("name", "") for a in entry.get("authors", [])[:5]
                )
                if len(entry.get("authors", [])) > 5:
                    authors += f" ç­‰ {len(entry.get('authors', []))} ä½ä½œè€…"

                arxiv_id = entry.get("id", "").split("/abs/")[-1]

                all_papers.append({
                    "id": arxiv_id,
                    "title": clean_text(entry.get("title", "")),
                    "summary": truncate_text(clean_text(entry.get("summary", ""))),
                    "url": f"https://arxiv.org/abs/{arxiv_id}",
                    "pdf_url": f"https://arxiv.org/pdf/{arxiv_id}.pdf",
                    "authors": authors,
                    "published": parse_arxiv_date(entry.get("published", "")),
                    "published_raw": entry.get("published", ""),
                    "category": category,
                })

        except Exception as e:
            print(f"   âš ï¸  è·å–å¤±è´¥: {e}")
            continue

    # å»é‡å¹¶æ’åº
    seen_ids = set()
    unique_papers = []
    for p in all_papers:
        if p["id"] not in seen_ids:
            seen_ids.add(p["id"])
            unique_papers.append(p)

    unique_papers.sort(key=lambda x: x.get("published_raw", ""), reverse=True)
    return unique_papers[:max_papers]


def create_issues(papers: list[dict], repo_name: str, token: str) -> int:
    """åˆ›å»º GitHub Issues"""
    if not papers:
        return 0

    g = Github(token)
    repo = g.get_repo(repo_name)

    existing_titles = {issue.title for issue in repo.get_issues(state='all')}
    created = 0

    for paper in papers:
        title = f"[è®ºæ–‡è®¨è®º] {paper['title']}"

        if title in existing_titles:
            print(f"â­ï¸  å·²å­˜åœ¨: {title[:50]}...")
            continue

        body = f"""## ğŸ“„ è®ºæ–‡ä¿¡æ¯

**æ ‡é¢˜**: [{paper['title']}]({paper['url']})
**ä½œè€…**: {paper['authors']}
**å‘å¸ƒæ—¶é—´**: {paper['published']}
**åˆ†ç±»**: {paper['category']}
**PDF**: [Download]({paper['pdf_url']})

## ğŸ“ æ‘˜è¦

{paper['summary']}

## ğŸ’¬ è®¨è®º

è¯·å¯¹è¿™ç¯‡è®ºæ–‡å‘è¡¨æ‚¨çš„è§è§£ï¼š
- è®ºæ–‡çš„åˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
- æ–¹æ³•æ˜¯å¦åˆç†ï¼Ÿ
- å®éªŒç»“æœæ˜¯å¦å¯ä¿¡ï¼Ÿ
- æœ‰å“ªäº›å¯ä»¥æ”¹è¿›çš„åœ°æ–¹ï¼Ÿ

@Moderator è¯·åˆ†è¯Š

---
_ç”± arXiv Monitor è‡ªåŠ¨åˆ›å»º_"""

        repo.create_issue(title=title, body=body)
        created += 1
        print(f"âœ… åˆ›å»º: {title[:50]}...")
        time.sleep(2)

    return created


def main(argv: list[str] | None = None) -> int:
    parser = argparse.ArgumentParser(description="arXiv Monitor - è·å–è®ºæ–‡å¹¶åˆ›å»º Issue")
    parser.add_argument("--token", type=str, help="GitHub Token")
    parser.add_argument("--repo", type=str, help="Repository (owner/repo)")
    parser.add_argument("--categories", type=str, default="cs.AI,cs.LG,cs.CL")
    parser.add_argument("--max-papers", type=int, default=5)
    parser.add_argument("--output", type=str, help="Output JSON file (optional)")
    parser.add_argument("--last-scan", type=str, help="Last scan time (ISO format)")

    args = parser.parse_args(argv)

    # é»˜è®¤ 7 å¤©å‰
    last_scan = args.last_scan or (
        datetime.now() - datetime.timedelta(days=7)
    ).strftime("%Y-%m-%dT%H:%M:%SZ")

    categories = [c.strip() for c in args.categories.split(",") if c.strip()]

    print(f"ğŸ” æ‰«æ arXiv...")
    print(f"   åˆ†ç±»: {', '.join(categories)}")
    print(f"   ä¸Šæ¬¡æ‰«æ: {last_scan}")

    # è·å–è®ºæ–‡
    papers = fetch_papers(categories, last_scan, args.max_papers)
    print(f"\nğŸ“Š å‘ç° {len(papers)} ç¯‡æ–°è®ºæ–‡")

    # ä¿å­˜ JSONï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(papers, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ä¿å­˜åˆ°: {args.output}")

    # åˆ›å»º Issuesï¼ˆå¦‚æœæä¾› Tokenï¼‰
    if args.token and args.repo:
        print(f"\nğŸ“„ åˆ›å»º Issues...")
        created = create_issues(papers, args.repo, args.token)
        print(f"\nğŸ‰ å®Œæˆï¼åˆ›å»º {created} ä¸ª Issues")
    else:
        for i, p in enumerate(papers, 1):
            print(f"   {i}. [{p['category']}] {p['title'][:50]}...")

    return 0


if __name__ == "__main__":
    sys.exit(main())
