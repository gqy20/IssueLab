#!/usr/bin/env python3
"""
arXiv Monitor - è·å–æ–°è®ºæ–‡ï¼Œæ™ºèƒ½åˆ†æï¼Œæ¨èè®¨è®º

Usage:
    # è·å–è®ºæ–‡å¹¶æ™ºèƒ½åˆ†æ
    python scripts/monitor_arxiv.py \
        --token "ghp_xxx" \
        --repo "owner/repo" \
        --categories "cs.AI,cs.LG,cs.CL"

    # ä»…æ‰«æè·å–è®ºæ–‡åˆ—è¡¨
    python scripts/monitor_arxiv.py --scan-only --output /tmp/papers.json
"""

import argparse
import json
import re
import sys
import time
from datetime import datetime
from typing import Any

import feedparser
from github import Github


def parse_arxiv_date(date_str: str) -> str:
    """è§£æ arXiv æ—¥æœŸæ ¼å¼"""
    try:
        dt = datetime.strptime(date_str[:19], "%Y-%m-%dT%H:%M:%S")
        return dt.strftime("%Y-%m-%d")
    except (ValueError, TypeError):
        return date_str[:10] if date_str else "Unknown"


def clean_text(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ä¸­çš„å¤šä½™ç©ºç™½"""
    return re.sub(r"\s+", " ", text).strip()


def truncate_text(text: str, max_length: int = 1500) -> str:
    """æˆªæ–­æ–‡æœ¬"""
    if len(text) <= max_length:
        return text
    return text[:max_length].rsplit(".", 1)[0] + "..."


def fetch_papers(categories: list[str], last_scan: str, max_papers: int = 10) -> list[dict[str, Any]]:
    """è·å– arXiv æ–°è®ºæ–‡"""
    try:
        last_scan_dt = datetime.strptime(last_scan[:19], "%Y-%m-%dT%H:%M:%S")
        last_scan_timestamp = last_scan_dt.timestamp()
    except (ValueError, TypeError):
        last_scan_timestamp = 0

    all_papers = []

    for category in categories:
        print(f"ğŸ“¥ è·å– {category} åˆ†ç±»...")

        base_url = "http://export.arxiv.org/api/query"
        params = {
            "search_query": f"cat:{category}",
            "sortBy": "submittedDate",
            "sortOrder": "descending",
            "max_results": max_papers * 3,
        }
        url = f"{base_url}?{ '&'.join(f'{k}={v}' for k,v in params.items()) }"

        try:
            response = feedparser.parse(url)

            for entry in response.entries:
                try:
                    published_timestamp = datetime.strptime(
                        entry.get("published", "")[:19], "%Y-%m-%dT%H:%M:%S"
                    ).timestamp()
                except (ValueError, TypeError):
                    continue

                if published_timestamp <= last_scan_timestamp:
                    continue

                authors = ", ".join(
                    a.get("name", "") for a in entry.get("authors", [])[:5]
                )
                if len(entry.get("authors", [])) > 5:
                    authors += f" ç­‰ {len(entry.get('authors', []))} ä½ä½œè€…"

                arxiv_id = entry.get("id", "").split("/abs/")[-1]

                all_papers.append({
                    "id": arxiv_id,
                    "title": clean_text(entry.get("title", "")),
                    "summary": truncate_text(clean_text(entry.get("summary", ""))),
                    "url": f"https://arxiv.org/abs/{arxiv_id}",
                    "pdf_url": f"https://arxiv.org/pdf/{arxiv_id}.pdf",
                    "authors": authors,
                    "published": parse_arxiv_date(entry.get("published", "")),
                    "published_raw": entry.get("published", ""),
                    "category": category,
                })

        except Exception as e:
            print(f"   âš ï¸  è·å–å¤±è´¥: {e}")
            continue

    # å»é‡å¹¶æ’åº
    seen_ids = set()
    unique_papers = []
    for p in all_papers:
        if p["id"] not in seen_ids:
            seen_ids.add(p["id"])
            unique_papers.append(p)

    unique_papers.sort(key=lambda x: x.get("published_raw", ""), reverse=True)
    return unique_papers[:max_papers]


def build_papers_for_observer(papers: list[dict]) -> str:
    """æ„å»ºä¾› Observer åˆ†æçš„è®ºæ–‡ä¸Šä¸‹æ–‡"""
    lines = ["## å¯è®¨è®ºçš„ arXiv è®ºæ–‡å€™é€‰\n"]

    for i, paper in enumerate(papers):
        lines.append(f"### è®ºæ–‡ {i}")
        lines.append(f"**æ ‡é¢˜**: {paper['title']}")
        lines.append(f"**åˆ†ç±»**: {paper['category']}")
        lines.append(f"**å‘å¸ƒæ—¶é—´**: {paper['published']}")
        lines.append(f"**é“¾æ¥**: [{paper['url']}]({paper['url']})")
        lines.append(f"**ä½œè€…**: {paper['authors']}")
        lines.append(f"**æ‘˜è¦**: {paper['summary']}")
        lines.append("")

    return "\n".join(lines)


def analyze_with_observer(papers: list[dict], papers_context: str, token: str) -> list[dict]:
    """ä½¿ç”¨ Observer agent åˆ†æè®ºæ–‡ï¼Œè¿”å›æ¨èçš„è®ºæ–‡"""
    # æ„å»º Observer çš„ç³»ç»Ÿæç¤º
    observer_prompt = """ä½ æ˜¯ IssueLab çš„ Observer Agentï¼Œè´Ÿè´£åˆ†æ arXiv è®ºæ–‡å¹¶æ¨èå€¼å¾—è®¨è®ºçš„è®ºæ–‡ã€‚

## æ¨¡å¼ 1ï¼šarXiv è®ºæ–‡åˆ†æ

å½“æ¥æ”¶ arXiv è®ºæ–‡åˆ—è¡¨æ—¶ï¼Œåˆ†æå¹¶æ¨èå€¼å¾—è®¨è®ºçš„è®ºæ–‡ã€‚

### å†³ç­–æ ‡å‡†

é€‰æ‹©è®ºæ–‡æ—¶è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š

| ç»´åº¦ | è¯´æ˜ | æ¨èæ ‡å‡† |
|------|------|---------|
| **ç ”ç©¶çƒ­åº¦** | çƒ­é—¨æ–¹å‘ï¼ˆLLMã€CVã€NLPï¼‰ | ä¼˜å…ˆ |
| **åˆ›æ–°æ€§** | æ–°æ–¹æ³•ã€æ–°æ€è·¯ | ä¼˜å…ˆ |
| **å®ç”¨æ€§** | å¼€æºã€å¤ç°æ€§å¥½ | ä¼˜å…ˆ |
| **æ—¶æ•ˆæ€§** | æœ€æ–°å‘å¸ƒ | ä¼˜å…ˆ |
| **äº‰è®®æ€§** | æœ‰è®¨è®ºç©ºé—´ | ä¼˜å…ˆ |

### è¾“å‡ºæ ¼å¼

è¯·è¾“å‡º YAML æ ¼å¼çš„æ¨èç»“æœï¼š

```yaml
analysis: |
  å…±æ”¶åˆ° X ç¯‡å€™é€‰è®ºæ–‡ï¼Œç»è¿‡åˆ†æåæ¨è Y ç¯‡å€¼å¾—è®¨è®ºã€‚

  ç®€è¦åˆ†æï¼š
  - è®ºæ–‡0ï¼šxxx
  - è®ºæ–‡1ï¼šxxx

recommended:
  - index: 0
    title: è®ºæ–‡æ ‡é¢˜
    reason: "æ¨èç†ç”±ï¼ˆç ”ç©¶æ–¹å‘çƒ­åº¦ + åˆ›æ–°ç‚¹ï¼‰"
    summary: "è®ºæ–‡æ‘˜è¦ï¼ˆç”¨äº Issue ä»‹ç»ï¼Œ100å­—å·¦å³ï¼‰"
```

### æ¨èç­–ç•¥

- æ¯æ‰¹è®ºæ–‡æœ€å¤šæ¨è 2-3 ç¯‡
- ä¼˜å…ˆé€‰æ‹©ä¸åŒæ–¹å‘çš„è®ºæ–‡ï¼Œé¿å…ä¸»é¢˜é‡å¤
- å¦‚æœè®ºæ–‡è´¨é‡æ™®éè¾ƒé«˜ï¼Œå¯æ¨èå…¨éƒ¨
- å¦‚æœè®ºæ–‡è´¨é‡æ™®éè¾ƒä½ï¼Œå¯å°‘äº 2 ç¯‡

## å½“å‰ä»»åŠ¡

è¯·åˆ†æä»¥ä¸‹å€™é€‰è®ºæ–‡ï¼Œæ¨èå€¼å¾—åˆ›å»º Issue è®¨è®ºçš„è®ºæ–‡ï¼š
"""

    # ä½¿ç”¨ Claude API åˆ†æï¼ˆç®€åŒ–å®ç°ï¼šè¿”å›å‰2ç¯‡ï¼‰
    # å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œåº”è¯¥è°ƒç”¨ Claude API
    # ç”±äºå½“å‰æ¶æ„é™åˆ¶ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€å•çš„å¯å‘å¼è§„åˆ™

    print(f"\nğŸ§  åˆ†æè®ºæ–‡ä¸­...")

    # ç®€å•å¯å‘å¼è§„åˆ™é€‰æ‹©è®ºæ–‡
    recommended = []
    selected_topics = set()

    for i, paper in enumerate(papers):
        # è·³è¿‡å·²è¢«é€‰è¿‡ç›¸åŒåˆ†ç±»çš„
        if paper['category'] in selected_topics and len(selected_topics) >= 2:
            continue

        # é€‰æ‹©å‰ 2 ç¯‡ä¸åŒåˆ†ç±»çš„è®ºæ–‡
        if len(recommended) < 2:
            # ä¼˜å…ˆé€‰æ‹©æ‘˜è¦ä¸­åŒ…å«çƒ­é—¨å…³é”®è¯çš„è®ºæ–‡
            hot_keywords = ['transformer', 'llm', 'diffusion', 'reinforcement', 'gpt', 'neural']
            summary_lower = paper['summary'].lower()
            hot_count = sum(1 for kw in hot_keywords if kw in summary_lower)

            reason = f"æœ€æ–°å‘å¸ƒçš„ {paper['category']} è®ºæ–‡"
            if hot_count > 0:
                reason = f"{paper['category']} çƒ­é—¨æ–¹å‘è®ºæ–‡ï¼ŒåŒ…å« {hot_count} ä¸ªçƒ­ç‚¹å…³é”®è¯"

            recommended.append({
                "index": i,
                "title": paper['title'],
                "reason": reason,
                "summary": paper['summary'][:200] + "...",
                "category": paper['category'],
                "url": paper['url'],
                "pdf_url": paper['pdf_url'],
                "authors": paper['authors'],
                "published": paper['published'],
            })

            selected_topics.add(paper['category'])

    print(f"âœ… åˆ†æå®Œæˆï¼Œæ¨è {len(recommended)} ç¯‡è®ºæ–‡")

    return recommended


def create_issues(recommended: list[dict], repo_name: str, token: str) -> int:
    """æ ¹æ® Observer æ¨èåˆ›å»º GitHub Issues"""
    if not recommended:
        print("ğŸ“­ æ— æ¨èè®ºæ–‡ï¼Œä¸åˆ›å»º Issue")
        return 0

    g = Github(token)
    repo = g.get_repo(repo_name)

    # è·å–å·²å­˜åœ¨çš„ Issue æ ‡é¢˜
    existing_titles = {issue.title for issue in repo.get_issues(state='all')}
    created = 0

    for paper in recommended:
        title = f"[è®ºæ–‡è®¨è®º] {paper['title']}"

        if title in existing_titles:
            print(f"â­ï¸  å·²å­˜åœ¨: {title[:50]}...")
            continue

        body = f"""## ğŸ“„ è®ºæ–‡ä¿¡æ¯

**æ ‡é¢˜**: [{paper['title']}]({paper['url']})
**ä½œè€…**: {paper['authors']}
**å‘å¸ƒæ—¶é—´**: {paper['published']}
**åˆ†ç±»**: {paper['category']}
**PDF**: [Download]({paper['pdf_url']})

## ğŸ“ ç®€ä»‹

{paper['summary']}

## ğŸ’¬ æ¨èç†ç”±

{paper['reason']}

## è®¨è®º

è¯·å¯¹è¿™ç¯‡è®ºæ–‡å‘è¡¨æ‚¨çš„è§è§£ï¼š
- è®ºæ–‡çš„åˆ›æ–°ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
- æ–¹æ³•æ˜¯å¦åˆç†ï¼Ÿ
- å®éªŒç»“æœæ˜¯å¦å¯ä¿¡ï¼Ÿ
- æœ‰å“ªäº›å¯ä»¥æ”¹è¿›çš„åœ°æ–¹ï¼Ÿ

---
_ç”± arXiv Monitor è‡ªåŠ¨åˆ›å»º_"""

        # åˆ›å»º Issue
        issue = repo.create_issue(title=title, body=body)
        print(f"âœ… åˆ›å»º Issue: {title[:50]}...")

        # åˆ›å»ºè¯„è®ºè§¦å‘ @Moderatorï¼ˆè¯„è®ºä¸­çš„ @ ä¼šè§¦å‘ orchestrator.ymlï¼‰
        trigger_comment = "@Moderator è¯·åˆ†è¯Š"
        issue.create_comment(trigger_comment)
        print(f"ğŸ“ è§¦å‘è¯„è®º: {trigger_comment}")

        created += 1
        time.sleep(2)

    return created


def main(argv: list[str] | None = None) -> int:
    parser = argparse.ArgumentParser(description="arXiv Monitor - æ™ºèƒ½è·å–å¹¶åˆ†æè®ºæ–‡")
    parser.add_argument("--token", type=str, help="GitHub Token")
    parser.add_argument("--repo", type=str, help="Repository (owner/repo)")
    parser.add_argument("--categories", type=str, default="cs.AI,cs.LG,cs.CL")
    parser.add_argument("--max-papers", type=int, default=10, help="è·å–è®ºæ–‡æ•°é‡ï¼ˆåˆ†æå‰ï¼‰")
    parser.add_argument("--output", type=str, help="Output JSON file (optional)")
    parser.add_argument("--last-scan", type=str, help="Last scan time (ISO format)")
    parser.add_argument("--scan-only", action="store_true", help="Only scan, don't analyze")

    args = parser.parse_args(argv)

    # é»˜è®¤ 7 å¤©å‰
    last_scan = args.last_scan or (
        datetime.now() - datetime.timedelta(days=7)
    ).strftime("%Y-%m-%dT%H:%M:%SZ")

    categories = [c.strip() for c in args.categories.split(",") if c.strip()]

    print(f"ğŸ” æ‰«æ arXiv...")
    print(f"   åˆ†ç±»: {', '.join(categories)}")
    print(f"   ä¸Šæ¬¡æ‰«æ: {last_scan}")

    # è·å–è®ºæ–‡
    papers = fetch_papers(categories, last_scan, args.max_papers)
    print(f"\nğŸ“Š å‘ç° {len(papers)} ç¯‡æ–°è®ºæ–‡")

    if not papers:
        print("ğŸ“­ æœªå‘ç°æ–°è®ºæ–‡")
        return 0

    # ä¿å­˜ JSONï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.output:
        with open(args.output, "w", encoding="utf-8") as f:
            json.dump(papers, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ä¿å­˜åˆ°: {args.output}")

    # ä»…æ‰«ææ¨¡å¼
    if args.scan_only:
        for i, p in enumerate(papers, 1):
            print(f"   {i}. [{p['category']}] {p['title'][:50]}...")
        return 0

    # åˆ†æå¹¶åˆ›å»º Issues
    if args.token and args.repo:
        # æ„å»ºä¸Šä¸‹æ–‡
        papers_context = build_papers_for_observer(papers)

        # Observer åˆ†æ
        recommended = analyze_with_observer(papers, papers_context, args.token)

        # åˆ›å»º Issues
        print(f"\nğŸ“„ åˆ›å»º Issues...")
        created = create_issues(recommended, args.repo, args.token)
        print(f"\nğŸ‰ å®Œæˆï¼åˆ›å»º {created} ä¸ª Issues")
    else:
        print("â„¹ï¸  æä¾› --token å’Œ --repo å‚æ•°å¯è‡ªåŠ¨åˆ†æå¹¶åˆ›å»º Issues")
        for i, p in enumerate(papers, 1):
            print(f"   {i}. [{p['category']}] {p['title'][:50]}...")

    return 0


if __name__ == "__main__":
    sys.exit(main())
